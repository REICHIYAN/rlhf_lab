from dataclasses import dataclass

from transformers import PreTrainedModel, PreTrainedTokenizerBase

from .datasets import PreferenceDataset
from .dpo_trainer import DPOTrainer, DPOConfig


@dataclass
class RLAIFConfig:
    """For simplicity, we reuse the DPOConfig."""
    lr: float = 1e-5
    batch_size: int = 2
    num_epochs: int = 1
    max_length: int = 128
    beta: float = 0.1


class RLAIFTrainer:
    """RLAIF trainer wrapper.

    In this toy implementation, RLAIF is equivalent to DPO, with the understanding
    that the preference dataset may have been generated by an AI judge rather than humans.
    """

    def __init__(
        self,
        model: PreTrainedModel,
        tokenizer: PreTrainedTokenizerBase,
        dataset: PreferenceDataset,
        config: RLAIFConfig,
    ):
        self.inner = DPOTrainer(
            model=model,
            tokenizer=tokenizer,
            dataset=dataset,
            config=DPOConfig(
                lr=config.lr,
                batch_size=config.batch_size,
                num_epochs=config.num_epochs,
                max_length=config.max_length,
                beta=config.beta,
            ),
        )

    def train(self) -> PreTrainedModel:
        return self.inner.train()
